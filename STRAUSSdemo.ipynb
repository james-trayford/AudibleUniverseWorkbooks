{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/james-trayford/AudibleUniverseWorkbooks/blob/group4/STRAUSSdemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXMXgH-xVgaX"
   },
   "source": [
    "Notebook prepared by **Dr James Trayford** - for queries please email [`james.trayford@port.ac.uk`](mailto:james.trayford@port.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBwYdDYB9lCQ"
   },
   "source": [
    "To use this notebook (if you haven't already) you can first save a copy to your local drive by clicking `File > Save a Copy in Drive` and run on that copy. `Edit > Clear all outputs` on that copy should also ensure yopu have a clean version\n",
    " to start from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v47s4t152sZ6"
   },
   "source": [
    "## **0. Introduction**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EMw6AfO200J"
   },
   "source": [
    "We will be using the [STRAUSS code](https://github.com/james-trayford/strauss) for this activity\n",
    "\n",
    "<img src=\"https://github.com/james-trayford/strauss/blob/main/misc/strauss_logo.png?raw=true\">\n",
    "\n",
    "For reference, you can read an overview of the code (as well as detailed documentation) [at this link](https://strauss.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ7CisIn22sg"
   },
   "source": [
    "`strauss` is an open source, object-oriented python library intended to be a flexible toolkit and engine for sonification, allowing detailed low-level control over the sonification process. Simultaneously, casual users can quickly hear their data, adapting a library of python notebook templates for a range of applications. \n",
    "\n",
    "By analogy to visualisation, the intention is to provide something akin to a plotting library. A library allows users to make a variety of simple plots easily, but also the option to control all aspects of plots and adapt them to the intricacies of their data, for optimal presentation. \n",
    "\n",
    "By adopting a general approach, `strauss` is intended to sonify any form of data for users with differing expertise. `strauss` is work in progress, and benefits form user feedback - filling in this feedback will be very useful in making the code better!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m0bIZRs7BaC"
   },
   "source": [
    "### **This notebook:** \n",
    "This notebook will demonstrate some of the ways in which `strauss` can be applied to sonify light curves. Alternative options may be demonstrated with commented out code ( i.e. lines of actual code preceded by `#`) - feel free to try these! Generally the goal of this notebook is to give some open examples to explore the code and experiment, so please do so! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMh0Lo6E8lc9"
   },
   "source": [
    "### **STRAUSS video**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQHg0a5Z5jB_"
   },
   "source": [
    "You can also run the below cell to see a 12 minute introduction video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVzULp-G4ZMz"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('yhSNM8ztSEk', width=800, height=600) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6ScbV3u_gLr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zJMkFJwTTC9"
   },
   "source": [
    "## **1. Setup:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH3ld6PZP2vX"
   },
   "source": [
    "First, let's install `strauss`! Just run the code cell below.\n",
    "\n",
    "*We will use the `spectraliser` development branch for this notebook. Install can take a while - but you should only need to run it once!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgF2VVpZPl_V"
   },
   "outputs": [],
   "source": [
    " !pip --quiet install git+https://github.com/james-trayford/strauss.git@spectraliser -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrRwGGvJdIP6"
   },
   "source": [
    "and also make a local copy of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqHJTrrTdBhB"
   },
   "outputs": [],
   "source": [
    " !git clone https://github.com/james-trayford/strauss.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4FQXd9fUkfr"
   },
   "source": [
    "Make plots appear in-line by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9_unXRNUgim"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYvyH45vTR-b"
   },
   "source": [
    "Import the modules we need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0rPodqYQorM"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# strauss imports\n",
    "from strauss.sonification import Sonification\n",
    "from strauss.sources import Events, Objects\n",
    "from strauss import channels\n",
    "from strauss.score import Score\n",
    "from strauss.generator import Sampler, Synthesizer, Spectralizer\n",
    "from strauss import sources as Sources \n",
    "import strauss\n",
    "\n",
    "# other useful modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import savgol_filter\n",
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "# modules to display in-notebook\n",
    "import IPython.display as ipd\n",
    "from IPython.core.display import display, Markdown, Latex, Image\n",
    "\n",
    "# set figures to be a decent size by default\n",
    "import matplotlib\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('figure', **{'figsize':[14.0, 7.0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kUJV_oI8mqD"
   },
   "source": [
    "## **2. Getting the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCFYP7KUP462"
   },
   "source": [
    "âš   ***The cells in this section are to organise the data we need for the session, don't worry too much about the details of the code here!*** âš  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nykq-9yBJAfE"
   },
   "source": [
    "First, let's download the data for this session to somewhere we have `Colab` access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cNnI7xN9lTM"
   },
   "outputs": [],
   "source": [
    "outdir = \"./AU2_Group4\"\n",
    "\n",
    "path = os.path.realpath(outdir)\n",
    "if not glob.glob(outdir): \n",
    "  os.mkdir(path)  \n",
    "    \n",
    "fname = \"Group4_input_data.zip\"\n",
    "url = \"https://drive.google.com/uc?export=download&id=1hZYDckpHnWWdZXOAavFOcieeZP42WGJr\"\n",
    "\n",
    "print(f\"Downloading files...\")\n",
    "with urllib.request.urlopen(url) as response, open(f\"{path}/{fname}\", 'wb') as out_file:\n",
    "  data = response.read() # a `bytes` object\n",
    "  out_file.write(data)\n",
    "\n",
    "print(f\"Unzipping files to {outdir}/Data ...\")\n",
    "with zipfile.ZipFile(f\"{outdir}/{fname}\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(f\"{outdir}\")\n",
    "\n",
    "print(f\"Clearing up...\")\n",
    "os.remove(f\"{path}/{fname}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGluXHJHYGqS"
   },
   "source": [
    "We now have access to all the data for this group - let's see what's available and make some plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAbG8PNrYDof"
   },
   "outputs": [],
   "source": [
    "for f in glob.glob('./AU2_Group4/Data/csv/*.csv'):\n",
    "  data_table = pd.read_csv(f)\n",
    "  display(Markdown(f\"### File: <mark>`{f}`</mark>\"))\n",
    "  xlab = data_table.columns[0]\n",
    "  ylab = data_table.columns[1]\n",
    "  plt.plot(data_table[xlab], data_table[ylab])\n",
    "  plt.xlabel(f'{xlab}')\n",
    "  plt.ylabel(f'{ylab}')\n",
    "  plt.show()\n",
    "  print(data_table.head())\n",
    "  #display(data_table)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO4y_pIS0_No"
   },
   "source": [
    "## **3. One-dimensional time series sonification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF0wzdSYjvp7"
   },
   "source": [
    "Here we will sonify observed light-curves as a ***one-dimensional time series*** , where some ***sound property*** is is varied with ***time*** in the ***sonification***, in the same way that **flux density**, varies with ***time*** in a ***light curve*** (early in the sonification is earlier observation time, later in the sonification is later observation time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCE-nk67Lf5N"
   },
   "source": [
    "### 3.1 Trying the `Events` source function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4DnjcHnSsLd"
   },
   "source": [
    "In `strauss` we could treat each ***flux density*** data point in the light curve as separate audio `Events`, with an occurence `time` mapped from their ***time***. \n",
    "\n",
    "However, articulating each data point as a separate ***note*** for ***many thousands*** of data points can require long and drawn-out sonifications. \n",
    "\n",
    "Here we demonstrate this approach with the light curves. We use the `Synthesizer` object with the `pitch_mapper` preset by default - this has a default pitch range of ***two octaves*** (a factor of 4 in frequency) and we pick an `E3` note (165 Hz) as the base (lowest) frequency.\n",
    "\n",
    "We will hear the **flux** of each point as `pitch`, with their time mapped to the occurence `time` (moving from low to high time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0ibVd2VLdzd"
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Sonifying in 1D using '`Events`' object:\"))\n",
    "\n",
    "# read in the various data file - can uncomment lines to hear other files - the last\n",
    "# uncommented `data_table` line will be the displayed file. \n",
    "data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/GALEX_NUV_LC.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/tic_lc.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/kid11616200_lc.csv\", delimiter=',')[1:]\n",
    "\n",
    "# grab times and fluxes from chosen data file\n",
    "time = data_table[:,0]\n",
    "flux = data_table[:,1]\n",
    "\n",
    "# show light curve again, for reference\n",
    "plt.scatter(time, flux, s=4)\n",
    "plt.ylabel('Flux Density')\n",
    "plt.xlabel('Time')\n",
    "plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "# specify the base notes used. In this example we use a single E3 note and \n",
    "# freely vary the pitch via the 'pitch_shift' parameter \n",
    "notes = [[\"E3\"]]\n",
    "\n",
    "# we could also just specify a particular frequency...\n",
    "#notes = [[150.]]\n",
    "\n",
    "score =  Score(notes, 30)\n",
    "\n",
    "\n",
    "data = {'pitch':np.ones(flux.size),\n",
    "        'time': time,\n",
    "        'pitch_shift':flux}\n",
    "\n",
    "# specify audio system (e.g. mono, stereo, 5.1, ...)\n",
    "system = \"mono\"\n",
    "\n",
    "# set up synth (this generates the sound using mathematcial waveforms)\n",
    "generator = Synthesizer()\n",
    "generator.load_preset('pitch_mapper')\n",
    "\n",
    "# or maybe the sampler instead by uncommenting this block (this uses recorded audio clips)\n",
    "#generator = Sampler(sampfiles=\"./strauss/data/samples/mallets\")\n",
    "#generator.modify_preset({'phi': 0,'theta':0,})\n",
    "\n",
    "generator.modify_preset({'note_length':0.15,\n",
    "                         'volume_envelope': {'use':'on',\n",
    "                                             'D':0.1,\n",
    "                                             'S':0.,\n",
    "                                             'A':0.01}})\n",
    "\n",
    "# set 0 to 100 percentile limits so the full pitch range is used...\n",
    "# setting 0 to 101 for pitch means the sonification is 1% longer than \n",
    "# the time needed to trigger each note - by making this more than 100%\n",
    "# we give all the notes time to ring out (setting this at 100% means\n",
    "# the final note is triggered at the momement the sonification ends)\n",
    "lims = {'time': ('0','101'),\n",
    "        'pitch_shift': ('0','100')}\n",
    "\n",
    "# set up source\n",
    "sources = Events(data.keys())\n",
    "sources.fromdict(data)\n",
    "sources.apply_mapping_functions(map_lims=lims)\n",
    "\n",
    "soni = Sonification(score, sources, generator, system)\n",
    "soni.render()\n",
    "dobj = soni.notebook_display()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWkgTFk3T5rL"
   },
   "source": [
    "The articulation of each note lets you hear each data point separately, but over many noisy data points can lead to a confusing result - it's hard to keep track of the fluxes and our pitch memory is challenged.\n",
    "\n",
    "Instead we can try ***smoothly evolving*** parameters, designating the light curves as an ***evolving `Object`*** source class. We demonstrate this in the following subsection `3.1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvWzJ2JrlrHP"
   },
   "source": [
    "### 3.2 Instead Trying the `Objects` Source Type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohlCzjEysV3N"
   },
   "source": [
    "Let's first try the evolving `Object` approach without modifying the raw data in any way. In analogy to visual display, the `Event` representation is like plotting the light curve as a scatter plot, while an `Object` representation is like plotting the light curve as a continuous line.\n",
    "\n",
    " We set up some parameters for `strauss`, e.g. choosing a sonification length of `40` seconds. A longer sonification might let you hear more detail, but will take longer to listen to (and for `Colab` to load!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nD7c6QR1NiJ"
   },
   "outputs": [],
   "source": [
    "# specify audio system (e.g. mono, stereo, 5.1, ...)\n",
    "system = \"mono\"\n",
    "\n",
    "# length of the sonification in s\n",
    "length = 40.\n",
    "\n",
    "# set up synth and turn on LP filter\n",
    "generator = Synthesizer()\n",
    "generator.load_preset('pitch_mapper')\n",
    "generator.preset_details('pitch_mapper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woJrlbVGcsmP"
   },
   "source": [
    "Let's pick a light curve to sonify (as in Section 2). We will again default to ***Type 1*** light curve `'51788-0386-086'`.\n",
    "\n",
    "Try changing this if you want to explore different light curves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMp2Wq5Ocmq1"
   },
   "outputs": [],
   "source": [
    "# read in the various data file - can uncomment lines to hear other files - the last\n",
    "# uncommented `data_table` line will be the displayed file. \n",
    "data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/GALEX_NUV_LC.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/tic_lc.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/kid11616200_lc.csv\", delimiter=',')[1:]\n",
    "\n",
    "# grab times and fluxes from chosen data file\n",
    "time = data_table[:,0]\n",
    "flux = data_table[:,1]\n",
    "\n",
    "# Finally, plot the light curve to remind us what we're working with\n",
    "plt.plot(time,flux)\n",
    "plt.xlabel(f'Time')\n",
    "plt.ylabel(f'Flux')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0K2YmC4C18a"
   },
   "source": [
    "let's hear the 1D time-series data sonification, mapping flux density to pitch for the light curve.\n",
    "\n",
    "*NB: If you were wondering why `strauss` refers to the varying pitch mapping as `pitch_shift` and not just `pitch`, this is because all sources in `strauss` also need a base `pitch` which is chosen from the `'notes'` structure (here always `'A2'`) by the `Score`. This is because `strauss` can play many sources at the same time! Again, you can read more about this [in the docs](https://strauss.readthedocs.io/en/latest/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeBqkWEk1Zny"
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Sonifying in 1D using '`pitch_shift`':\"))\n",
    "\n",
    "# show light curve again, for reference\n",
    "plt.plot(time,flux)\n",
    "plt.xlabel(f'Time')\n",
    "plt.ylabel(f'Flux')\n",
    "plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "notes = [[\"A3\"]]\n",
    "score =  Score(notes, length)\n",
    "\n",
    "data = {'pitch':1.,\n",
    "        'time_evo':time,\n",
    "        'pitch_shift':flux}\n",
    "\n",
    "# set 0 to 100 percentile limits so the full pitch and time range is used...\n",
    "lims = {'time_evo': ('0','100'),\n",
    "        'pitch_shift': ('0','100')}\n",
    "\n",
    "# set up source\n",
    "sources = Objects(data.keys())\n",
    "sources.fromdict(data)\n",
    "sources.apply_mapping_functions(map_lims=lims)\n",
    "\n",
    "soni = Sonification(score, sources, generator, system)\n",
    "soni.render()\n",
    "dobj = soni.notebook_display()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAmJKKnlM-80"
   },
   "source": [
    "How about mapping a low-pass filter to flux density? \n",
    "\n",
    "*using a 'tonal' carrier, uncomment line for a 'textural' (or white noise) carrier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn1UPAZxEXy0"
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Sonifying in 1D using low-pass '`cutoff`' frequency:\"))\n",
    "\n",
    "# show light curve again, for reference\n",
    "plt.plot(time,flux)\n",
    "plt.xlabel(f'Time')\n",
    "plt.ylabel(f'Flux')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "generator = Synthesizer()\n",
    "generator.modify_preset({'filter':'on'})\n",
    "\n",
    "# uncomment these lines to try a 'textural' sonification using white noise!\n",
    "# generator.load_preset('windy')\n",
    "# generator.preset_details('windy')\n",
    "\n",
    "# we use a (power!) 'chord' here to create more harmonic richness...\n",
    "notes = [[\"A2\", \"E3\", 'A3', 'E4']]\n",
    "score =  Score(notes, length)\n",
    "\n",
    "data = {'pitch':[0,1,2,3],\n",
    "        'time_evo':[time]*4,\n",
    "        'cutoff':[flux]*4}\n",
    "\n",
    "lims = {'time_evo': ('0','100'),\n",
    "        'cutoff': ('0','100')}\n",
    "\n",
    "# set up source\n",
    "sources = Objects(data.keys())\n",
    "sources.fromdict(data)\n",
    "plims = {'cutoff': (0.25,0.9)}\n",
    "sources.apply_mapping_functions(map_lims=lims, param_lims=plims)\n",
    "\n",
    "soni = Sonification(score, sources, generator, system)\n",
    "soni.render()\n",
    "dobj = soni.notebook_display()\n",
    "\n",
    "# change back in case cells are run out of order...\n",
    "generator.load_preset('pitch_mapper')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOVerUAeZlAh"
   },
   "source": [
    "In fact, there are many expressive properties of sound we could use to represent the data in a similar way. In `strauss` these are referred to as `mappable` properties. \n",
    "\n",
    "A subset of these can be used as an evolving property with the `Object` source class. These are referred to as `evolvable` properties. \n",
    "\n",
    "Lets show what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvhfpdWZZjPt"
   },
   "outputs": [],
   "source": [
    "display(Markdown(f\"### ***'Mappable'*** properties:\"))\n",
    "for m in Sources.mappable:\n",
    "  display(Markdown(f' * `{m}` '))\n",
    "\n",
    "display(Markdown(f\"### ***'Evolvable'*** properties:\"))\n",
    "for m in Sources.evolvable:\n",
    "  display(Markdown(f' * `{m}` '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6napS2OIt6K"
   },
   "source": [
    "We can play around with some of these `evolvable` properties here. Are all of these effective for this data? Could they be effective for other types of data representations?\n",
    "\n",
    "The `idx` variable below controls which evolvable property is selected from the `some_mappings` list. `idx` can be changed to a number from 0 to 5 (inclusive) to *'index'* a certain sound parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs5lkyB5njUe"
   },
   "outputs": [],
   "source": [
    "# A list of some 'evolvable' mappings\n",
    "some_mappings = [\"volume\", \n",
    "                 \"phi\",\n",
    "                 \"volume_lfo/amount\", \n",
    "                 \"volume_lfo/freq_shift\",\n",
    "                 \"pitch_lfo/amount\", \n",
    "                 \"pitch_lfo/freq_shift\"]\n",
    "\n",
    "# change this (between 0 and 5) to select a different property to map...\n",
    "idx = 1\n",
    "\n",
    "# use a stereo system to allow 'phi' mapping (low pan left and high pan right)\n",
    "system = \"stereo\"\n",
    "\n",
    "display(Markdown(f\"### Sonifying in 1D using `evolvable` property - ***`{some_mappings[idx]}`***:\"))\n",
    "\n",
    "# show light curve again, for reference\n",
    "plt.plot(time,flux)\n",
    "plt.xlabel(f'Time')\n",
    "plt.ylabel(f'Flux')\n",
    "plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "generator = Synthesizer()\n",
    "generator.modify_preset({'filter':'on',\n",
    "                         \"pitch_hi\":-1, \"pitch_lo\": 1,\n",
    "                         \"pitch_lfo\": {\"use\": \"on\", \n",
    "                                       \"amount\":1*(\"pitch_lfo/freq_shift\" == some_mappings[idx]), \n",
    "                                       \"freq\":3*5**(\"pitch_lfo/freq_shift\" != some_mappings[idx]), \n",
    "                                       \"phase\":0.25},\n",
    "                         \"volume_lfo\": {\"use\": \"on\", \n",
    "                                        \"amount\":1*(\"volume_lfo/freq_shift\" == some_mappings[idx]), \n",
    "                                        \"freq\":3*5**(\"volume_lfo/freq_shift\" != some_mappings[idx]), \n",
    "                                        \"phase\":0}\n",
    "                        })\n",
    "\n",
    "# try a different chord (stacking fifths)...\n",
    "notes = [[\"A2\",\"E3\",\"B4\",\"F#4\"]]\n",
    "\n",
    "# chords and can also be specified via chord names and base octave....\n",
    "# notes = \"Em6_3\"\n",
    "\n",
    "\n",
    "score =  Score(notes, length)\n",
    "\n",
    "data = {'pitch':[0,1,2,3],\n",
    "        'time_evo':[time]*4,\n",
    "        'cutoff':[0.9]*4,\n",
    "        'theta':[0.5]*4,\n",
    "        some_mappings[idx]:[(flux - flux.min())/(flux.max()-flux.min())]*4}\n",
    "\n",
    "lims = {'time_evo': ('0','100'),\n",
    "        \"volume\": ('0','100'), \n",
    "        \"phi\": (-0.5,1.5),\n",
    "        \"volume_lfo/amount\": ('0','100'), \n",
    "        \"volume_lfo/freq_shift\": ('0','100'),\n",
    "        \"pitch_lfo/amount\": ('0','100'), \n",
    "        \"pitch_lfo/freq_shift\": ('0','100')}\n",
    "\n",
    "# set up source\n",
    "sources = Objects(data.keys())\n",
    "sources.fromdict(data)\n",
    "plims = {'cutoff': (0.25,0.9)}\n",
    "sources.apply_mapping_functions(map_lims=lims, param_lims=plims)\n",
    "\n",
    "soni = Sonification(score, sources, generator, system)\n",
    "soni.render()\n",
    "dobj = soni.notebook_display()\n",
    "%matplotlib inline\n",
    "\n",
    "# change back in case cells are run out of order...\n",
    "generator.load_preset('pitch_mapper')\n",
    "system = 'mono'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox6vDc4qIB1A"
   },
   "source": [
    "### 3.X A Side Note About Presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9eHQQ_cGzwm"
   },
   "source": [
    "You can see all the parameters that make up the default `Synthesizer` preset in a pop-up window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_uxJlqqoZOU"
   },
   "outputs": [],
   "source": [
    "%pycat /usr/local/lib/python3.7/dist-packages/strauss/presets/synth/default.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsqFGB6ZG_yJ"
   },
   "source": [
    "... as well as the other presets we might want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSvkP1eMooke"
   },
   "outputs": [],
   "source": [
    "ls -1 /usr/local/lib/python3.7/dist-packages/strauss/presets/synth/*.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irfXngv4HJpe"
   },
   "source": [
    "We can modify these presets at runtime (as we do in various examples throughout this session), or even write our own presets in `.yml` format.\n",
    "\n",
    "The `generator.preset()` function also accepts a filepath to a custom presets, where any changed preset parameters replace those in the `default` preset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svu0wV5akl24"
   },
   "source": [
    "## **4. 'Musical' example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous examples, we have allowed pitch to freely vary between data points. In this example, we try ***binning the pitches*** onto musical scale intervals to make a more ***'musical'*** (in the cultural context of Western music theory) example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we return to the `Event` based sonification, where we now use the quantised `'pitch'` parameter (as opposed to the continuous `'pitch_shift` parameter) to represent flux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can comment and uncomment code in this cell to try some alternatives. What scale works best? What `pitch_binning` mode? Do different approaches work better for different data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Sonifying in 1D using '`Events`' object:\"))\n",
    "\n",
    "# read in the various data file - can uncomment lines to hear other files - the last\n",
    "# uncommented `data_table` line will be the displayed file. \n",
    "data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/GALEX_NUV_LC.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/tic_lc.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/kid11616200_lc.csv\", delimiter=',')[1:]\n",
    "\n",
    "# grab times and fluxes from chosen data file\n",
    "time = data_table[:,0]\n",
    "flux = data_table[:,1]\n",
    "\n",
    "# show light curve again, for reference\n",
    "plt.scatter(time, flux, s=4)\n",
    "plt.ylabel('Flux Density')\n",
    "plt.xlabel('Time')\n",
    "plt.show()\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "# specify the notes used. \n",
    "\n",
    "# C major pentatonic\n",
    "notes = [[\"C3\",\"E3\",\"F3\",\"G3\",\"B3\",\"C4\",\"E4\",\"F4\",\"G4\",\"B4\",\"C5\",\"E5\",\"F5\",\"G5\",\"B5\"]]\n",
    "\n",
    "# or a whole-tone scale, (was it just a dream... a dream... a dream...)\n",
    "#\n",
    "# This is a symetrical scale and here also demonstrates using raw frequencies in Hz.\n",
    "# The whole tone scale uses 2 semitone jumps, what about minor thirds (3) fourths (5)\n",
    "# or fifths (7)? Specify `semitones` to change this.\n",
    "# `nint` specifies the number of intervals in the scale. Note a bigger semitone interval\n",
    "# will reach higher pitches (perhaps inaudible ones...)\n",
    "\n",
    "# semitones = 2.\n",
    "# nint = 10\n",
    "# notes = [100*2**(np.arange(nint)*(semitones/12))]\n",
    "\n",
    "# ------\n",
    "# In this context, we also consider the `Score` optional parameter `pitch_binning`.\n",
    "#\n",
    "# This determines how we bin the 'pitch' quantity, which can be either:\n",
    "# - 'uniform':  the range of mapped values for 'pitch' are split into even bins\n",
    "#               representing  each of the scored notes\n",
    "# - 'adaptive': the range of mapped values for 'pitch' split by percentile. \n",
    "#               such that each interval is played approximately the same number of \n",
    "#               times\n",
    "# without specifying,  'adaptive' is used by default, but we specify 'uniform' here\n",
    "#\n",
    "# which is better for representing this data?\n",
    "\n",
    "# we specify 'uniform' pitch binning in the primary example...\n",
    "score =  Score(notes, 30, pitch_binning='uniform')\n",
    "\n",
    "# what about adaptive?\n",
    "#score =  Score(notes, 30, pitch_binning='adaptive')\n",
    "\n",
    "data = {'pitch':flux,\n",
    "        'time': time}\n",
    "\n",
    "# specify audio system (e.g. mono, stereo, 5.1, ...)\n",
    "system = \"mono\"\n",
    "\n",
    "# set up synth (this generates the sound using mathematical waveforms)\n",
    "generator = Synthesizer()\n",
    "generator.load_preset('pitch_mapper')\n",
    "\n",
    "generator.modify_preset({'note_length':0.15,\n",
    "                         'volume_envelope': {'use':'on',\n",
    "                                             'D':0.1,\n",
    "                                             'S':0.,\n",
    "                                             'A':0.01,\n",
    "                                             'R':0}})\n",
    "\n",
    "# set 0 to 100 percentile limits so the full pitch range is used...\n",
    "# setting 0 to 101 for pitch means the sonification is 1% longer than \n",
    "# the time needed to trigger each note - by making this more than 100%\n",
    "# we give all the notes time to ring out (setting this at 100% means\n",
    "# the final note is triggered at the momement the sonification ends)\n",
    "lims = {'time': ('0','101'),\n",
    "        'pitch_shift': ('0','100'),\n",
    "        'pitch': ('0','100')}\n",
    "\n",
    "# set up source\n",
    "sources = Events(data.keys())\n",
    "sources.fromdict(data)\n",
    "sources.apply_mapping_functions(map_lims=lims)\n",
    "\n",
    "soni = Sonification(score, sources, generator, system)\n",
    "soni.render()\n",
    "dobj = soni.notebook_display()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNTjcJNolrGS"
   },
   "source": [
    "## ***5. Bonus*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we try some bonus material if there's time, including ***spectral representations*** and ***multivariate data***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.1. Spectral Representation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prKB8Zaml9lU"
   },
   "source": [
    "ðŸš§ ***under construction*** ðŸš§\n",
    "\n",
    "For this type of data, a more unorthodox representation could be a ***spectral representation.***\n",
    "Here, we will represent the light curves as spectra, using the `Spectralizer` generator class in `strauss` to generate a sound signal with this spectral form.\n",
    "\n",
    "A complexity with this approach is that ***positive features*** are much more clearly represented than ***negative features*** or a ***continuum***. For this reason it makes sense to subtract some representative value (to *\"zero the continuum\"*) and then represent postive or negative features above it. In the example below, we can demonstrate spectralizing the light curves as-is, or for super- and sub-median brightness features only. *Do any of these effectively represent the data? Do different light curves require different approaches?*\n",
    "\n",
    "Again, there are a number of commented code blocks in the below example that demonstrate this.\n",
    "\n",
    "*\"Spectralisation\"* is covered in much more detail in the other group notebook [here](https://github.com/james-trayford/AudibleUniverseWorkbooks/blob/group3/STRAUSSdemo.ipynb)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Sonifying using spectral representation:\"))\n",
    "\n",
    "# read in the various data file - can uncomment lines to hear other files - the last\n",
    "# uncommented `data_table` line will be the displayed file. \n",
    "data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/GALEX_NUV_LC.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/tic_lc.csv\", delimiter=',')[1:]\n",
    "#data_table = np.genfromtxt(\"./AU2_Group4/Data/csv/kid11616200_lc.csv\", delimiter=',')[1:]\n",
    "\n",
    "# grab times and fluxes from chosen data file\n",
    "time = data_table[:,0]\n",
    "flux = data_table[:,1]\n",
    "\n",
    "# compute the super- and sub-median features of the light curve\n",
    "flux_med = np.percentile(flux, 50)\n",
    "flux_hi = np.clip(flux, flux_med, np.inf) - flux_med\n",
    "flux_lo = flux_med - np.clip(flux, -np.inf, np.percentile(flux, 50))\n",
    "\n",
    "# raising the flux to a power ('contrast') of > 1 can accentuate peaks.\n",
    "contrast = 1.\n",
    "#contrast = 2.\n",
    "#contrast = 4.\n",
    "\n",
    "# specify the base notes used. In this example we use a single E3 note and \n",
    "# freely vary the pitch via the 'pitch_shift' parameter \n",
    "notes = [[\"E3\"]]\n",
    "\n",
    "score =  Score(notes, 6)\n",
    "\n",
    "data = {'pitch': [1],\n",
    "        'volume_envelope/D':[0.6], \n",
    "        'volume_envelope/S':[0.], \n",
    "        'volume_envelope/A':[0.05]}\n",
    "\n",
    "# set the spectrum (raised by 'contrast').  \n",
    "data['spectrum'] = [(flux)**contrast]\n",
    "#data['spectrum'] = [(flux_hi)**contrast]\n",
    "#data['spectrum'] = [(flux_lo)**contrast]\n",
    "\n",
    "\n",
    "# specify audio system (e.g. mono, stereo, 5.1, ...)\n",
    "system = \"mono\"\n",
    "\n",
    "# set up synth (this generates the sound using mathematcial waveforms)\n",
    "generator = Spectralizer()\n",
    "\n",
    "# we can modify the frequency range (in Hz) like this: \n",
    "#generator.modify_preset({'min_freq':100, 'max_freq':2000})\n",
    "\n",
    "# Illustrate with some plots (can ignore the details here)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(121)\n",
    "bdx = abs((flux-flux_med)**contrast) == abs(data['spectrum'][0])\n",
    "plt.plot(time, flux, label='Light Curve')\n",
    "if bdx.any():\n",
    "    plt.scatter(time[bdx], flux[bdx],c='C1', label='Sonified data points')\n",
    "plt.legend(frameon=0)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Flux')\n",
    "ax = fig.add_subplot(122)\n",
    "plt.plot(np.linspace(generator.preset['min_freq'], \n",
    "                     generator.preset['max_freq'],\n",
    "                     flux.size), data['spectrum'][0], \n",
    "         label='Sonified Spectrum')\n",
    "plt.legend(frameon=0)\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.show()\n",
    "\n",
    "# set 0 to 100 percentile limits so the full pitch range is used...\n",
    "# setting 0 to 101 for pitch means the sonification is 1% longer than \n",
    "lims = {'frequency': ('0','100'),\n",
    "        'spectrum': ('0','100')}\n",
    "\n",
    "# set up source\n",
    "sources = Events(data.keys())\n",
    "sources.fromdict(data)\n",
    "sources.apply_mapping_functions(map_lims=lims)\n",
    "\n",
    "%matplotlib notebook\n",
    "soni = Sonification(score, sources, generator, system)\n",
    "soni.render()\n",
    "dobj = soni.notebook_display()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.2. Multivariate Sonification***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above examples, we have considered a single value varying with an independent variable, but can we also try sonifying multivariate data? \n",
    "\n",
    "***For this there is a bonus notebook located [here](https://github.com/james-trayford/AudibleUniverseWorkbooks/blob/group3/STRAUSSdemo_bonus.ipynb)***!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKyd+9gNFtgcL4U2O08zwu",
   "collapsed_sections": [
    "MCE-nk67Lf5N",
    "ox6vDc4qIB1A"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
